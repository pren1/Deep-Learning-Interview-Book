[TOC]

# 机器学习

## 逻辑回归（LR）

### 基本原理

[逻辑回归（Logistic Regression，LR）](https://en.wikipedia.org/wiki/Logistic_regression)也称为"对数几率回归"，又称为"逻辑斯谛"回归。

**知识点提炼**

- **分类**，经典的二分类算法！
- 逻辑回归就是这样的一个过程：面对一个回归或者分类问题，建立代价函数，然后通过优化方法迭代求解出最优的模型参数，然后测试验证我们这个求解的模型的好坏。
- Logistic 回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别）
- 回归模型中，y 是一个定性变量，比如 y = 0 或 1，logistic 方法主要应用于研究某些事件发生的概率。
- 逻辑回归的本质：极大似然估计
- 推导：https://blog.csdn.net/The_lastest/article/details/78761577
- 逻辑回归的激活函数：Sigmoid
- 逻辑回归的代价函数：交叉熵

**逻辑回归的优缺点**

优点： 

1）速度快，适合二分类问题 

2）简单易于理解，直接看到各个特征的权重 

3）能容易地更新模型吸收新的数据 

缺点： 

对数据和场景的适应能力有局限性，不如决策树算法适应性那么强

**逻辑回归中最核心的概念是 [Sigmoid 函数](https://en.wikipedia.org/wiki/Sigmoid_function)**，Sigmoid函数可以看成逻辑回归的激活函数。

**Regression 常规步骤**

1. 寻找h函数（即预测函数）
2. 构造J函数（损失函数）
3. 想办法（迭代）使得J函数最小并求得回归参数（θ）

函数h(x)的值有特殊的含义，它表示结果取1的概率，于是可以看成类1的后验估计。因此对于输入x分类结果为类别1和类别0的概率分别为： 

P(y=1│x;θ)=hθ (x) 

P(y=0│x;θ)=1-hθ (x)

**代价函数**

**逻辑回归一般使用交叉熵作为代价函数**。关于[代价函数](https://en.wikipedia.org/wiki/Loss_function)的具体细节，请参考[代价函数](http://www.cnblogs.com/Belter/p/6653773.html)。

交叉熵是对「出乎意料」（译者注：原文使用suprise）的度量。神经元的目标是去计算函数 y, 且 y = y(x)。但是我们让它取而代之计算函数 a, 且 a = a(x) 。假设我们把 a 当作 y 等于 1 的概率，1−a 是 y 等于 0 的概率。那么，交叉熵衡量的是我们在知道 y 的真实值时的平均「出乎意料」程度。当输出是我们期望的值，我们的「出乎意料」程度比较低；当输出不是我们期望的，我们的「出乎意料」程度就比较高。

交叉熵代价函数如下所示：

$$J(w)=-l(w)=-\sum_{i = 1}^n y^{(i)}ln(\phi(z^{(i)})) + (1 - y^{(i)})ln(1-\phi(z^{(i)}))$$

$$J(\phi(z),y;w)=-yln(\phi(z))-(1-y)ln(1-\phi(z))$$

注：为什么要使用交叉熵函数作为代价函数，而不是平方误差函数？

答：如果我们采用传统的平方误差函数，其画出的代价函数图就不会是一个凸函数，而是有很多局部最优点

**逻辑回归伪代码**

```
初始化线性函数参数为1
构造sigmoid函数
重复循环I次
	计算数据集梯度
	更新线性函数参数
确定最终的sigmoid函数
输入训练（测试）数据集
运用最终sigmoid函数求解分类
```

**逻辑回归算法之Python实现**

```python
for i in range(max_iter):
  z = X*W + b
  error = sigmoid(z) - y
  W = W - 1/m*alpha*X.T*error
  b = b - 1/m*alpha*sum(error)
return W,b
```

**参考资料**

- [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)
- 《统计学习方法》 (蓝书)  第6章  P77页
- 《机器学习》 (西瓜书) 第3章  P57页
- [《Machine Learning》 吴恩达 Logistic Regression](https://d19vezwu8eufl6.cloudfront.net/ml/docs%2Fslides%2FLecture6.pdf)
- [逻辑回归 - 理论篇](https://blog.csdn.net/pakko/article/details/37878837)
- [逻辑回归(logistic regression)的本质——极大似然估计](https://blog.csdn.net/zjuPeco/article/details/77165974)
- [机器学习算法与Python实践之（七）逻辑回归（Logistic Regression）](https://www.cnblogs.com/zhizhan/p/4868555.html)
- [机器学习之Logistic回归与Python实现](https://blog.csdn.net/moxigandashu/article/details/72779856)
- [【机器学习】逻辑回归（Logistic Regression）](https://www.cnblogs.com/Belter/p/6128644.html)
- [机器学习算法--逻辑回归原理介绍](https://blog.csdn.net/chibangyuxun/article/details/53148005)
- [逻辑回归算法面经](https://zhuanlan.zhihu.com/p/46591702)
- [Logistic Regression 模型简介](https://tech.meituan.com/2015/05/08/intro-to-logistic-regression.html)

### 为什么 LR 要使用 sigmoid 函数？

1.广义模型推导所得
2.满足统计的最大熵模型
3.性质优秀，方便使用（Sigmoid函数是平滑的，而且任意阶可导，一阶二阶导数可以直接由函数值得到不用进行求导，这在实现中很实用）

**参考资料**

- [为什么逻辑回归 模型要使用 sigmoid 函数](https://blog.csdn.net/weixin_39881922/article/details/80366324)

### LR 可以用核函数么？

在计算决策面时，SVM算法里只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的）。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。
————————————————
版权声明：本文为CSDN博主「韦人人韦」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/ddydavie/article/details/82668141

### LR 能否解决非线性分类问题？

逻辑回归本质上是一个线性回归模型，但需要用kernel trick才能处理非线性问题。

**参考资料**

- [逻辑斯蒂回归能否解决非线性分类问题？](https://www.zhihu.com/question/29385169)

### LR为什么要离散特征？

稀疏向量内积乘法运算速度快

离散数据对异常数据有很强的鲁棒性

单变量离散化为 N 个后，每个变量有单独的权重，在激活函数的作用下相当于为模型增加了非线性，能够提升模型表达能力，加大拟合

[阅读](https://testerhome.com/topics/11064/show_wechat)

> LR 是一个非常简单的线性模型。 我们再次回顾它的公式：y = w*x + b。 这是一个线性函数。 我们之前说 0 过， 线性函数的表达能力有限， 我们引入激活函数就是为了给 LR 增加非线性关系。能让一条直线变成曲线。这样可以拟合出更好的效果。（也由此才后了后来说的过拟合问题而引入了正则化超参数）， 那么离散化和连续化最大的区别是，对一个字段做连续化后的结果就还只是一个特征，而离散化后的这一列有多少个 key(字段可能的值) 就会抽取出多少个特征。 那么第一点就来了， 单变量离散化为 N 个后，每个变量有单独的权重，在激活函数的作用下相当于为模型增加了非线性，能够提升模型表达能力，加大拟合。 第二点，离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30 是 1，否则 0。如果特征没有离散化，一个异常数据 “年龄 300 岁” 会给模型造成很大的干扰， 因为特征值的异常会导致权重也就是 w 的值也会异常。第三，离散特征的增加和减少都很容易，易于模型的快速迭代。 第四， 一定有同学担心特征过多会导致运算缓慢，但是 LR 是线性模型， 我们在内部计算的时候是向量化计算，而不是循环迭代。稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。 所以不用担心像 GBDT 算法那样，特征多了就跑不动了 (我们都说 GBDT 不能用离散特征不是因为它处理不了离散特征，而是因为离散化特征后会产生特别多的特征，决策树的叶子节点过多，遍历的时候太慢了)。 所以海量离散特征＋LR 是业内常见的一个做法。而少量连续特征 + 复杂模型是另外一种做法，例如 GBDT。

## 线性回归

### 线性回归与逻辑回归（LR）的区别

线性回归用来回归，输出连续数值。逻辑回归用来分类

可以认为逻辑回归的输入是线性回归的输出，将逻辑斯蒂函数（Sigmoid曲线）作用于线性回归的输出得到输出结果。

**参考资料**

- [线性回归和逻辑回归的比较](https://blog.csdn.net/ddydavie/article/details/82668141)

## 支持向量机（SVM）

### 基本原理

[支持向量机（supporr vector machine，SVM）](https://en.wikipedia.org/wiki/Support-vector_machine)是一种二类分类模型，该模型是定义在特征空间上的间隔最大的线性分类器。间隔最大使它有区别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。支持向量机的学习策略就是**间隔最大化**，可形式化为一个求解凸二次规划的最小化问题。

**知识点提炼：**

- SVM核函数
  - 多项式核函数
  - 高斯核函数
  - 字符串核函数
- SMO （求解SVM对偶函数的一个算法）
- SVM损失函数

支持向量机的学习算法是求解凸二次规划的最优化算法。

支持向量机学习方法包含构建由简至繁的模型：

- 线性可分支持向量机
- 近似线性可分支持向量机
- 非线性支持向量机（使用核函数）

当训练数据线性可分时，通过硬间隔最大化（hard margin maximization）学习一个线性的分类器，即线性可分支持向量机，又成为硬间隔支持向量机；

当训练数据近似线性可分时，通过软间隔最大化（soft margin maximization）也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；

当训练数据不可分时，通过核技巧（kernel trick）及软间隔最大化，学习非线性支持向量机。

**注：以上各SVM的数学推导应该熟悉：硬间隔最大化（几何间隔）---学习的对偶问题---软间隔最大化（引入松弛变量）---非线性支持向量机（核技巧）**。

**SVM的主要特点**

（1）非线性映射-理论基础 

（2）最大化分类边界-方法核心 

（3）支持向量-计算结果 

（4）小样本学习方法（指分类平面只有一小部分样本起作用，如下一条所说）

（5）最终的决策函数只有少量支持向量决定，避免了“维数灾难” 

（6）少数支持向量决定最终结果—->可“剔除”大量冗余样本+算法简单+具有鲁棒性（体现在3个方面） 

（7）学习问题可表示为凸优化问题—->全局最小值 

（8）可自动通过最大化边界控制模型，但需要用户指定核函数类型和引入松弛变量（也就是软间隔）

（9）适合于小样本，优秀泛化能力（因为结构风险最小） 

（10）泛化错误率低，分类速度快，结果易解释

**SVM为什么采用间隔最大化？**

当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。

感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。

线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。

然后应该借此阐述，几何间隔，函数间隔，及从函数间隔—>求解最小化1/2 ||w||^2 时的w和b。即线性可分支持向量机学习算法—最大间隔法的由来。

**为什么要将求解SVM的原始问题转换为其对偶问题？**

1. 对偶问题往往更易求解（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。）
2. 自然引入核函数，进而推广到非线性分类问题

**为什么SVM要引入核函数？**

当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。

**SVM核函数有哪些？**

- 线性（Linear）核函数：主要用于线性可分的情形。参数少，速度快。
- 多项式核函数
- 高斯（RBF）核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。
- Sigmoid核函数
- 拉普拉斯（Laplac）核函数

注：如果feature数量很大，跟样本数量差不多，建议使用LR或者Linear kernel的SVM。如果feature数量较少，样本数量一般，建议使用Gaussian Kernel的SVM。

**SVM如何处理多分类问题？**

一般有两种做法：

1. 直接法：直接在目标函数上修改，将多个分类面的参数求解合并到一个最优化问题里面。看似简单但是计算量却非常的大。

2. 间接法：对训练器进行组合。其中比较典型的有一对一，和一对多。
   - 一对多：对每个类都训练出一个分类器，由svm是二分类，所以将此而分类器的两类设定为目标类为一类，其余类为另外一类。这样针对k个类可以训练出k个分类器，当有一个新的样本来的时候，用这k个分类器来测试，那个分类器的概率高，那么这个样本就属于哪一类。这种方法效果不太好，bias比较高。
   - 一对一：针对任意两个类训练出一个分类器，如果有k类，一共训练出C(2,k) 个分类器，这样当有一个新的样本要来的时候，用这C(2,k) 个分类器来测试，每当被判定属于某一类的时候，该类就加一，最后票数最多的类别被认定为该样本的类。

**SVM中硬间隔和软间隔**

硬间隔分类即线性可分支持向量机，软间隔分类即线性不可分支持向量机，利用软间隔分类时是因为存在一些训练集样本不满足函数间隔（泛函间隔）大于等于1的条件，于是加入一个非负的参数 ζ （松弛变量），让得出的函数间隔加上 ζ 满足条件。于是软间隔分类法对应的拉格朗日方程对比于硬间隔分类法的方程就多了两个参数（一个ζ ，一个 β），但是当我们求出对偶问题的方程时惊奇的发现这两种情况下的方程是一致的。下面我说下自己对这个问题的理解。

我们可以先考虑软间隔分类法为什么会加入ζ 这个参数呢？硬间隔的分类法其结果容易受少数点的控制，这是很危险的，由于一定要满足函数间隔大于等于1的条件，而存在的少数离群点会让算法无法得到最优解，于是引入松弛变量，从字面就可以看出这个变量是为了缓和判定条件，所以当存在一些离群点时我们只要对应给他一个ζi，就可以在不变更最优分类超平面的情况下让这个离群点满足分类条件。

综上，我们可以看出来软间隔分类法加入ζ 参数，使得最优分类超平面不会受到离群点的影响，不会向离群点靠近或远离，相当于我们去求解排除了离群点之后，样本点已经线性可分的情况下的硬间隔分类问题，所以两者的对偶问题是一致的。

### 支持向量中的向量是指什么？

指决定超平面的那几个点到超平面的向量

### 典中典之手推SVM（稍后再准备吧，先准备了也记不下）

**参考资料**

- [Support-vector machine](https://en.wikipedia.org/wiki/Support-vector_machine)
- [支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/v_july_v/article/details/7624837)
- [数据挖掘（机器学习）面试--SVM面试常考问题](https://blog.csdn.net/szlcw1/article/details/52259668)
- [机器学习实战教程（八）：支持向量机原理篇之手撕线性SVM](http://cuijiahua.com/blog/2017/11/ml_8_svm_1.html)
- [支持向量机（SVM）入门理解与推导](https://blog.csdn.net/sinat_20177327/article/details/79729551)
- [数据挖掘领域十大经典算法之—SVM算法（超详细附代码）](https://blog.csdn.net/fuqiuai/article/details/79483057)

### LR 与 SVM的区别和联系

**相同点**

第一，LR和SVM都是分类算法。

看到这里很多人就不会认同了，因为在很大一部分人眼里，LR是回归算法。我是非常不赞同这一点的，因为我认为判断一个算法是分类还是回归算法的唯一标准就是样本label的类型，如果label是离散的，就是分类算法，如果label是连续的，就是回归算法。很明显，LR的训练数据的label是“0或者1”，当然是分类算法。其实这样不重要啦，暂且迁就我认为它是分类算法吧，再说了，SVM也可以回归用呢。

第二，如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。

这里要先说明一点，那就是LR也是可以用核函数的，至于为什么通常在SVM中运用核函数而不在LR中运用，后面讲到他们之间区别的时候会重点分析。总之，原始的LR和SVM都是线性分类器，这也是为什么通常没人问你决策树和LR什么区别，决策树和SVM什么区别，你说一个非线性分类器和一个线性分类器有什么区别？

第三，LR和SVM都是监督学习算法。

这个就不赘述什么是监督学习，什么是半监督学习，什么是非监督学习了。

第四，LR和SVM都是判别模型。

判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率。简单来说，在计算判别模型时，不会计算联合概率，而在计算生成模型时，必须先计算联合概率。或者这样理解：生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。当然，这也是为什么很少有人问你朴素贝叶斯和LR以及朴素贝叶斯和SVM有什么区别（哈哈，废话是不是太多）。

**不同点**

第一，本质上是其损失函数（loss function）不同。

注：lr的损失函数是 cross entropy loss， adaboost的损失函数是 expotional loss ,svm是hinge loss，常见的回归模型通常用 均方误差 loss。

不同的loss function代表了不同的假设前提，也就代表了不同的分类原理，也就代表了一切！！！简单来说，​逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值，具体细节参考[逻辑回归](http://blog.csdn.net/pakko/article/details/37878837)。支持向量机​基于几何间隔最大化原理，认为存在最大几何间隔的分类面为最优分类面，具体细节参考[支持向量机通俗导论（理解SVM的三层境界）](http://blog.csdn.net/macyang/article/details/38782399)

第二，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。

当​你读完上面两个网址的内容，深入了解了LR和SVM的原理过后，会发现影响SVM决策面的样本点只有少数的结构支持向量，当在支持向量外添加或减少任何样本点对分类决策面没有任何影响；而在LR中，每个样本点都会影响决策面的结果。

理解了这一点，有可能你会问，然后呢？有什么用呢？有什么意义吗？对使用两种算法有什么帮助么？一句话回答：

因为上面的原因，得知：线性SVM不直接依赖于数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要先对数据做balancing。​（引自http://www.zhihu.com/question/26768865/answer/34078149）

第三，在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。

这个问题理解起来非常简单。分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM算法里只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的）。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。​

第四，​线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响。（引自http://www.zhihu.com/question/26768865/answer/34078149）

一个机遇概率，一个机遇距离！​

第五，SVM的损失函数就自带正则！！！（损失函数中的1/2||w||^2项），这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！

以前一直不理解为什么SVM叫做结构风险最小化算法，**所谓结构风险最小化，意思就是在训练误差和模型复杂度之间寻求平衡，防止过拟合，从而达到真实误差的最小化**。未达到结构风险最小化的目的，最常用的方法就是添加正则项，后面的博客我会具体分析各种正则因子的不同，这里就不扯远了。但是，你发现没，SVM的目标函数里居然自带正则项！！！

**快速理解LR和SVM的区别**

两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。但是逻辑回归相对来说模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些。但是SVM的理论基础更加牢固，有一套结构化风险最小化的理论基础，虽然一般使用的人不太会去关注。还有很重要的一点，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。

**SVM与LR的区别与联系**

联系：（1）分类（二分类） （2）可加入正则化项 

区别：（1）LR–参数模型；SVM–非参数模型？（2）目标函数：LR—logistical loss；SVM–hinge loss （3）SVM–support vectors；LR–减少较远点的权重 （4）LR–模型简单，好理解，精度低，可能局部最优；SVM–理解、优化复杂，精度高，全局最优，转化为对偶问题—>简化模型和计算 （5）LR可以做的SVM可以做（线性可分），SVM能做的LR不一定能做（线性不可分）

**总结一下**

- Linear SVM和LR都是线性分类器
- Linear SVM不直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要对数据先做balancing。
- Linear SVM依赖数据表打对距离测度，所以需要对数据先做normalization；LR不受影响
- Linear SVM依赖penalty的系数，实验中需要做validation
- Linear SVM的LR的performance都会收到outlier的影响，就敏感程度而言，无法给出明确结论。

**参考资料**

- [LR与SVM的异同](https://www.cnblogs.com/zhizhan/p/5038747.html)
- [SVM和logistic回归分别在什么情况下使用？](<https://www.zhihu.com/question/21704547/answer/20293255>)
- [Linear SVM 和 LR 有什么异同？](https://www.zhihu.com/question/26768865/answer/34078149)

- https://www.jianshu.com/p/e07932472257?utm_campaign)

### SMO 算法原理

- [ ] TODO

### SVM 为什么可以处理非线性问题？

因为用核方法可以把数据投影到高维，高维下数据说不定就可分了。

### SVM 中的优化技术有哪些？

- [ ] TODO

### SVM 的惩罚系数如何确定？

[阅读](https://7125messi.github.io/post/svm%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/)

SVM调优详解：交叉验证与grid search。具体更复杂的参数上升下降方法就再说～

### 正则化参数对支持向量数的影响

C小时，支持向量数少。C大时，会有更多的支持向量。

### Hinge Loss

SVM的loss是hinge loss

## 梯度提升树（GBDT）

### 基本原理

下面关于GBDT的理解来自论文greedy function approximation: a gradient boosting machine

1. 损失函数的数值优化可以看成是在函数空间，而不是在参数空间。
2. 损失函数L(y,F)包含平方损失(y−F)2，绝对值损失|y−F|用于回归问题，负二项对数似然log(1+e−2yF),y∈{-1,1}用于分类。
3. 关注点是预测函数的加性扩展。

最关键的点在于损失函数的数值优化可以看成是在函数空间而不是参数空间。

GBDT对分类问题基学习器是二叉分类树，对回归问题基学习器是二叉决策树。



1. 函数空间的优化：可以看作不断更新函数的过程。每个样本的训练值为损失函数对上一轮的函数求导。所以又叫gradient boost。

2. Bagging方法中，各学习器间相互独立，并且权重相同。例子：随机森林

3. Boosting算法中学习器有先后顺序，并且每一个样本都有其权重。

   > 初始时，每一个样本的权重是相等的。首先，第1 11个学习器对训练样本进行学习，当学习完成后，增大错误样本的权重，同时减小正确样本的权重，再利用第2 22个学习器对其进行学习，依次进行下去，最终得到b bb个学习器，最终，合并这b bb个学习器的结果
   > ————————————————
   > 版权声明：本文为CSDN博主「zhiyong_will」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
   > 原文链接：https://blog.csdn.net/google19890102/article/details/51746402/

4. This is done by building a new model on the errors or residuals of the previous model.

5. Gradient boosting 回归和分类的唯一区别在于损失函数。前者是MSE，后者是log- likelihood。

6. gradient boosting regression步骤

   1. 取所有结果的平均值作为模型输出。这是因为，对mse做最大似然估计，得到的最优解就是所有y的平均值。gradient boost开始于一个叶子结点
   2. 求pseudo residual，即模型输出与ground truth的差。
   3. 用决策树预测residual error。在每个叶子结点，决策树输出为叶子结点所有residual error的平均值。
   4. 更新模型，也就是把新的决策树乘以学习率再加到上一步的模型上。这样一看，所有的决策树都有统一的学习率，这是和adaboost的区别所在。

**参考资料**

- [简单易学的机器学习算法——梯度提升决策树GBDT](https://blog.csdn.net/google19890102/article/details/51746402/)

- [GBDT原理详解](https://www.cnblogs.com/ScorpioLu/p/8296994.html)

## AdaBoost

### 基本原理

Adaboost算法基本原理就是将多个弱分类器（弱分类器一般选用单层决策树）进行合理的结合，使其成为一个强分类器。

Adaboost采用迭代的思想，每次迭代只训练一个弱分类器，训练好的弱分类器将参与下一次迭代的使用。也就是说，在第N次迭代中，一共就有N个弱分类器，其中N-1个是以前训练好的，其各种参数都不再改变，本次训练第N个分类器。其中弱分类器的关系是第N个弱分类器更可能分对前N-1个弱分类器没分对的数据，最终分类输出要看这N个分类器的综合效果。

1. Adaboost一般使用单层决策树作为其弱分类器
2. 数据权重：分类错误的数据具有更大的权重
3. 分类器权重：分类器错误率越低，权重越高。最后时根据权重组合给出答案。

**参考资料**

- [Adaboost入门教程——最通俗易懂的原理介绍（图文实例）](https://blog.csdn.net/px_528/article/details/72963977)
- [AdaBoost原理详解](https://www.cnblogs.com/ScorpioLu/p/8295990.html)
- [数据挖掘领域十大经典算法之—AdaBoost算法（超详细附代码）](https://blog.csdn.net/fuqiuai/article/details/79482487)
- [聊聊Adaboost，从理念到硬核推导](https://zhuanlan.zhihu.com/p/62037189)

## XGBoost

### 基本原理

**XGBoost全名叫（eXtreme Gradient Boosting）极端梯度提升**，经常被用在一些比赛中，其效果显著。它是大规模并行boosted tree的工具，它是目前最快最好的开源boosted tree工具包。下面我们将XGBoost的学习分为3步：

① 集成思想 

② 损失函数分析 

③ 求解

我们知道机器学习三要素：模型、策略、算法。对于集成思想的介绍，XGBoost算法本身就是以集成思想为基础的。所以理解清楚集成学习方法对XGBoost是必要的，它能让我们更好的理解其预测函数模型。在第二部分，我们将详细分析损失函数，这就是我们将要介绍策略。第三部分，对于目标损失函数求解，也就是算法了。

**参考资料**

- [XGBoost Documentation](https://xgboost.readthedocs.io/en/latest/)
- [通俗、有逻辑的写一篇说下Xgboost的原理，供讨论参考](https://blog.csdn.net/github_38414650/article/details/76061893)
- [xgboost的原理没你想像的那么难](https://www.jianshu.com/p/7467e616f227)

### XGBoost里处理缺失值的方法

- [ ] TODO

### XGBoost 和 GBDT 的区别

- 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。

- 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。

- xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。

- Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）

- 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。

- 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。

- xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

- 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

### XGBoost 如何做到自定义损失函数？

- [ ] TODO

### XGBoost 如何防止过拟合？

- [ ] TODO

### XGBoost 为什么不用后剪枝？

- [ ] TODO

### XGBoost 是如何实现并行的？

- [ ] TODO

### XGBoost 有哪些参数，取指范围，各代表什么意思？

- [ ] TODO

**参考资料**

- [XGBoost——机器学习（理论+图解+安装方法+python代码）](https://blog.csdn.net/huacha__/article/details/81029680)
- [一文读懂机器学习大杀器 XGBoost 原理](http://blog.itpub.net/31542119/viewspace-2199549/)  

### XGBoost 如何进行并行加速的？

- [ ] TODO

### 每次分裂叶子节点是怎么决定特征和分裂点的？

- [ ] TODO

### Adaboost、GBDT与XGBoost的区别

- [ ] TODO

**参考资料**

- [Adaboost、GBDT与XGBoost的区别](https://blog.csdn.net/hellozhxy/article/details/82143554)

## LightGBM

### 基本原理

- [ ] TODO

### LightGBM 与 XGBoost 的区别

- [ ] TODO

### GBDT、LightGBM 和 XGBoost 区别

- [ ] TODO

### 基本原理

1、KNN算法概述

　　kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 
　　
2、KNN算法介绍

 　　最简单最初级的分类器是将全部的训练数据所对应的类别都记录下来，当测试对象的属性和某个训练对象的属性完全匹配时，便可以对其进行分类。但是怎么可能所有测试对象都会找到与之完全匹配的训练对象呢，其次就是存在一个测试对象同时与多个训练对象匹配，导致一个训练对象被分到了多个类的问题，基于这些问题呢，就产生了KNN。

KNN是通过测量不同特征值之间的距离进行分类。它的的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。K通常是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。

下面通过一个简单的例子说明一下：如下图，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。

![](https://images0.cnblogs.com/blog2015/771535/201508/041623504236939.jpg)
　
接下来对KNN算法的思想总结一下：就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：

1）计算测试数据与各个训练数据之间的距离；

2）按照距离的递增关系进行排序；

3）选取距离最小的K个点；

4）确定前K个点所在类别的出现频率；

5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。　

**参考资料**

- [KNN算法原理及实现](https://www.cnblogs.com/sxron/p/5451923.html)
- [第2章 k-近邻算法](https://www.cnblogs.com/apachecnxy/p/7462628.html)
- [数据挖掘领域十大经典算法之—K-邻近算法/kNN（超详细附代码）](https://blog.csdn.net/fuqiuai/article/details/79458648)

### KD树

每次把数据用某个特征分一半，这样就有一个树。

对一个新的数据，找到叶子节点之后，在叶子节点里找一个最近的点，距离为radius。以数据点为中心画圆。如果该圆与其他区域（叶子节点）相交，那么也搜索相交区域，看看可不可以找到最近的点。

## K-Means

### 基本原理

算法思想：

```
选择K个点作为初始质心  
repeat  
    将每个点指派到最近的质心，形成K个簇  
    重新计算每个簇的质心  
until 簇不发生变化或达到最大迭代次数  
```

这里的重新计算每个簇的质心，如何计算的是根据目标函数得来的，因此在开始时我们要考虑距离度量和目标函数。

考虑欧几里得距离的数据，使用误差平方和（Sum of the Squared Error,SSE）作为聚类的目标函数，两次运行K均值产生的两个不同的簇集，我们更喜欢SSE最小的那个。

**参考资料**

- [深入理解K-Means聚类算法](https://blog.csdn.net/taoyanqi8932/article/details/53727841)
- [数据挖掘领域十大经典算法之—K-Means算法（超详细附代码）](https://blog.csdn.net/fuqiuai/article/details/79458331)

### 手撕 K-Means

- [ ] TODO

### K-Means 与 KNN 的区别

KNN是分类监督式学习算法，K-mean是聚类算法

**参考资料**

- [Kmeans算法与KNN算法的区别](https://www.cnblogs.com/peizhe123/p/4619066.html)

### K-Means 中的 K 怎么确定？

肘方法：对n个点的数据集，计算k=[1,n]时的距离平方和。画出来，当有拐点存在时，选取该值为k。

### K-Means 的迭代循环停止条件

簇不发生变化或达到最大迭代次数

### 评判聚类效果准则

使用误差平方和

## Bagging

### 基本原理

- 先介绍Bagging方法：

  Bagging即套袋法，其算法过程如下：

  1. 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
  2. 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
  3. 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

**参考资料**

- [集成算法中的Bagging](https://blog.csdn.net/fontthrone/article/details/79074296)

### Boosting

 AdaBoosting方式每次使用的是全部的样本，每轮训练改变样本的权重。下一轮训练的目标是找到一个函数f 来拟合上一轮的残差。当残差足够小或者达到设置的最大迭代次数则停止。Boosting会减小在上一轮训练正确的样本的权重，增大错误样本的权重。（对的残差小，错的残差大）

   梯度提升的Boosting方式是使用代价函数对上一轮训练出的模型函数f的偏导来拟合残差。

### Bagging 和 Boosting 的区别

1）样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的.

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化.而权值是根据上一轮的分类结果进行调整.

2）样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大.

3）预测函数：

Bagging：所有预测函数的权重相等.

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重.

4）并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果.

**参考资料**

- [Bagging和Boosting的区别（面试准备）](https://www.cnblogs.com/earendil/p/8872001.html)

- [Bagging和Boosting 概念及区别](https://www.cnblogs.com/liuwu265/p/4690486.html)
- [Bagging和Boosting的概念与区别](https://www.cnblogs.com/onemorepoint/p/9264782.html)


## 朴素贝叶斯

**参考资料**

- [第4章 基于概率论的分类方法：朴素贝叶斯](https://www.cnblogs.com/apachecnxy/p/7471634.html)
- [数据挖掘领域十大经典算法之—朴素贝叶斯算法（超详细附代码）](https://blog.csdn.net/fuqiuai/article/details/79458943)

### 为什么朴素贝叶斯被称为“朴素”？

因为我们假设每个特征在已知y的条件下是独立的。P(x1,x2|y) = P(x1,y) * P(x2,y)

## EM 算法

### 基本原理

**E步骤：**根据参数初始值或上一次迭代的模型参数来计算出隐性变量的后验概率，其实就是隐性变量的期望。作为隐藏变量的现估计值

**M步骤：**将似然函数最大化以获得新的参数值

要最大化似然函数，此处E步固定参数，计算Q，即给定参数与训练数据x，获得z的后验概率。M步固定Q，优化参数。即，当z的后验概率可知时，求能让下界最大的theta。

**参考资料**

- [数据挖掘领域十大经典算法之—EM算法](https://blog.csdn.net/fuqiuai/article/details/79484421)

### E 步和 M 步的具体步骤

- [ ] TODO

### E 中的期望是什么？

- [ ] TODO

## 决策树

决策树是一种判别模型，既支持分类问题，也支持回归问题，是一种非线性模型（分段线性函数不是线性的）。它天然的支持多分类问题。

### 决策树如何剪枝？

预剪枝：若划分后验证集精度不大于划分前验证集精度，则停止划分。否则进行划分。

后剪枝：后剪枝自底向上的判断每一个非叶结点在剪枝前、后的**验证集**的精度，（剪枝后该非叶结点变为叶子结点，其类别为划分到该结点的所有**训练样本**中占比最多的类别），若剪枝后**验证集**在决策树上的精度大于剪枝前的精度，那么该非叶结点应该进行剪枝，否则予以保留。

### 决策树先剪枝还是后剪枝好？

后剪枝，预剪枝容易欠拟合

### 决策树能否有非数值型变量？

能，比如下雨或不下雨

### 决策树如何防止过拟合？

https://www.jianshu.com/p/fe94516d6f39

**参考资料**

- [机器学习之-常见决策树算法(ID3、C4.5、CART)](https://shuwoom.com/?p=1452)
- [机器学习实战（三）——决策树](https://blog.csdn.net/jiaoyangwm/article/details/79525237)
- [决策树基本概念及算法优缺点](https://www.jianshu.com/p/655d8e555494)

### 决策树的ID3和C4.5介绍一下

ID3：因此我们总是选择当前使得信息增益最大的特征来划分数据集。

信息增益会偏向取值较多的特征，使用信息增益比可以对这一问题进行校正。

C4.5: 使用信息增益比，剪枝算法

CART：使用Gini指数

> Step1：设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时，对每一个特征A，对其可能取的每个值a，根据样本点A=a的测试为“是”或“否”将D分割为D1和D2两部分，利用上式Gini(D,A)来计算A=a时的基尼指数。
>
> Step2：在所有可能的特征A以及他们所有可能的切分点a中，选择基尼指数最小的特征及其对应可能的切分点作为最有特征与最优切分点。依最优特征与最有切分点，从现结点生成两个子节点，将训练数据集依特征分配到两个子节点中去。

## 随机森林（RF）

### 基本原理

随机森林属于集成学习（Ensemble Learning）中的bagging算法。在集成学习中，主要分为bagging算法和boosting算法。我们先看看这两种方法的特点和区别。

**Bagging（套袋法）**

bagging的算法过程如下：

从原始样本集中使用Bootstraping方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集。（k个训练集之间相互独立，元素可以有重复）
对于k个训练集，我们训练k个模型（这k个模型可以根据具体问题而定，比如决策树，knn等）
对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果。（所有模型的重要性相同）

**Boosting（提升法）**

boosting的算法过程如下：

对于训练集中的每个样本建立权值wi，表示对每个样本的关注度。当某个样本被误分类的概率很高时，需要加大对该样本的权值。

进行迭代的过程中，每一步迭代都是一个弱分类器。我们需要用某种策略将其组合，作为最终模型。（例如AdaBoost给每个弱分类器一个权值，将其线性组合最为最终分类器。误差越小的弱分类器，权值越大）
Bagging，Boosting的主要区别

样本选择上：Bagging采用的是Bootstrap随机有放回抽样；而Boosting每一轮的训练集是不变的，改变的只是每一个样本的权重。

样本权重：Bagging使用的是均匀取样，每个样本权重相等；Boosting根据错误率调整样本权重，错误率越大的样本权重越大。

预测函数：Bagging所有的预测函数的权重相等；Boosting中误差越小的预测函数其权重越大。

并行计算：Bagging各个预测函数可以并行生成；Boosting各个预测函数必须按顺序迭代生成。

下面是将决策树与这些算法框架进行结合所得到的新的算法：

1）Bagging + 决策树 = 随机森林

2）AdaBoost + 决策树 = 提升树

3）Gradient Boosting + 决策树 = GBDT

**决策树**

常用的决策树算法有ID3，C4.5，CART三种。3种算法的模型构建思想都十分类似，只是采用了不同的指标。决策树模型的构建过程大致如下：

**ID3，C4.5决策树的生成**

输入：训练集D，特征集A，阈值eps 输出：决策树T

若D中所有样本属于同一类Ck，则T为单节点树，将类Ck作为该结点的类标记，返回T

若A为空集，即没有特征作为划分依据，则T为单节点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T
否则，计算A中各特征对D的信息增益(ID3)/信息增益比(C4.5)，选择信息增益最大的特征Ag

若Ag的信息增益（比）小于阈值eps，则置T为单节点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T
否则，**依照特征Ag将D划分为若干非空子集Di**，将Di中实例数最大的类作为标记，构建子节点，由结点及其子节点构成树T，返回T

对第i个子节点，以Di为训练集，以A-{Ag}为特征集，递归地调用1~5，得到子树Ti，返回Ti

**CART决策树的生成**

这里只简单介绍下CART与ID3和C4.5的区别。

CART树是二叉树，而ID3和C4.5可以是**多叉树**
CART在生成子树时，是选择一个特征一个取值作为切分点，生成两个子树
选择特征和切分点的依据是基尼指数，选择基尼指数最小的特征及切分点生成子树

**随机森林**

随机森林是一种重要的基于Bagging的集成学习方法，可以用来做分类、回归等问题。

随机森林有许多优点：

- 具有极高的准确率
- 随机性的引入，使得随机森林不容易过拟合
- 随机性的引入，使得随机森林有很好的抗噪声能力
- 能处理很高维度的数据，并且不用做特征选择
- 既能处理离散型数据，也能处理连续型数据，数据集无需规范化
- 训练速度快，可以得到变量重要性排序
- 容易实现并行化

**随机森林的缺点：**

当随机森林中的决策树个数很多时，训练时需要的空间和时间会较大

随机森林模型还有许多不好解释的地方，有点算个黑盒模型

与上面介绍的Bagging过程相似，随机森林的构建过程大致如下：

从原始训练集中使用Bootstraping方法**随机有放回采样**选出m个样本，共进行n_tree次采样，生成n_tree个训练集
对于n_tree个训练集，我们分别训练n_tree个决策树模型

对于单个决策树模型，假设训练样本特征的个数为n，那么每次分裂时根据信息增益/信息增益比/基尼指数选择最好的特征进行分裂

每棵树都一直这样分裂下去，直到该节点的所有训练样例都属于同一类。在决策树的分裂过程中不需要剪枝
将生成的多棵决策树组成随机森林。对于分类问题，按多棵树分类器投票决定最终分类结果；**对于回归问题，由多棵树预测值的均值决定最终预测结果**

**参考资料**

- [随机森林算法学习(Random Forest)](https://blog.csdn.net/qq547276542/article/details/78304454)
- [随机森林（Random Forest）算法原理](https://blog.csdn.net/edogawachia/article/details/79357844)
- [随机森林（Random Forest）](https://www.cnblogs.com/maybe2030/p/4585705.html)
- [机器学习算法之随机森林算法详解及工作原理图解](http://www.elecfans.com/d/647463.html)

### 随机森林中的“随机”指什么？

随机森林的随机性体现在每颗树的训练样本是随机的，树中每个节点的分裂属性集合也是随机选择确定的。有了这2个随机的保证，随机森林就不会产生过拟合的现象了。

### 随机森林处理缺失值的方法

现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：**（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上**

对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。

**即，选取没缺失的数据，计算比例，把比例乘到gain上面**

对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为：

**即，把该样本放到所有分支中，权值为每个分支所含的样本比例**

作者：黄粱梦醒
链接：https://www.jianshu.com/p/fe94516d6f39
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

### 随机森林和 GBDT 的区别

1. 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。
2. 组成随机森林的树可以并行生成；而GBDT只能是串行生成。
3. 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。
4. 随机森林对异常值不敏感；GBDT对异常值非常敏感。

## 机器学习中的分类、回归和聚类模型有哪些？  

分类：LR、SVM、KNN、决策树、RandomForest、GBDT  

回归：non-Linear regression、SVR（支持向量回归-->可用线性或高斯核（RBF））、随机森林  

聚类：Kmeans、层次聚类、GMM（高斯混合模型）、谱聚类

## 高斯混合模型（GMM）

- [ ] TODO

**参考资料**

- [深度理解高斯混合模型（GMM）](http://blog.sina.com.cn/s/blog_a36a563e0102y2ec.html)
- [高斯混合模型（GMM）](https://blog.csdn.net/m_buddy/article/details/80432384)

## 马尔科夫随机场（MRF）

### 基本原理

- [ ] TODO

**参考资料**

- [马尔可夫随机场 MRF](https://blog.csdn.net/pipisorry/article/details/78396503)

## 隐马尔科夫模型（HMM）

### 基本原理

- [ ] TODO

**参考资料**

- [一文搞懂HMM（隐马尔可夫模型）](https://www.cnblogs.com/skyme/p/4651331.html)
- [机器学习中的隐马尔科夫模型（HMM）详解](https://blog.csdn.net/baimafujinji/article/details/51285082)

### 发射概率和状态转移概率

- [ ] TODO

### 每层要记住所有路径吗？

- [ ] TODO

## 条件随机场（CRF）

### 基本原理

- [ ] TODO

### CRF 的损失函数是什么？

- [ ] TODO

**参考资料**

- [机器学习之条件随机场（CRF）](https://blog.csdn.net/wangyangzhizhou/article/details/78489593)
- [如何轻松愉快地理解条件随机场（CRF）？](https://www.jianshu.com/p/55755fc649b1)
- [一文理解条件随机场CRF](https://zhuanlan.zhihu.com/p/70067113)

### HMM、MEMM vs CRF 对比？

1）HMM是有向图模型，是生成模型；HMM有两个假设：一阶马尔科夫假设和观测独立性假设；但对于序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。

2）MEMM（最大熵马尔科夫模型）是有向图模型，是判别模型；MEMM打破了HMM的观测独立性假设，MEMM考虑到相邻状态之间依赖关系，且考虑整个观察序列，因此MEMM的表达能力更强；但MEMM会带来标注偏置问题：由于局部归一化问题，MEMM倾向于选择拥有更少转移的状态。这就是标记偏置问题。

![img](https://pic3.zhimg.com/80/v2-7a5e998530e0d9c5f146d27603e6e496_hd.jpg)最大熵模型（MEMM）

![img](https://pic3.zhimg.com/80/v2-610ca7a9b504936bfba136c464ebe81a_hd.jpg)

3）CRF模型解决了标注偏置问题，去除了HMM中两个不合理的假设，当然，模型相应得也变复杂了。

HMM、MEMM和CRF的优缺点比较：

a）与HMM比较。CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样）

b）与MEMM比较。由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。

c）与ME比较。CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布.

> 首先，CRF，HMM(隐马模型)，MEMM(最大熵隐马模型)都常用来做序列标注的建模，像分词、词性标注，以及命名实体标注
> 隐马模型一个最大的缺点就是由于其输出独立性假设，导致其不能考虑上下文的特征，限制了特征的选择
> 最大熵隐马模型则解决了隐马的问题，可以任意选择特征，但由于其在每一节点都要进行归一化，所以只能找到局部的最优值，同时也带来了标记偏见的问题，即凡是训练语料中未出现的情况全都忽略掉。
> 条件随机场则很好的解决了这一问题，他并不在每一个节点进行归一化，而是所有特征进行全局归一化，因此可以求得全局的最优值。

**参考资料**

- [HMM、MEMM vs CRF 对比？](https://zhuanlan.zhihu.com/p/57153934)

## 主成分分析（PCA）

### 基本原理

PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。网上关于PCA的文章有很多，但是大多数只描述了PCA的分析过程，而没有讲述其中的原理。这篇文章的目的是介绍PCA的基本数学原理，帮助读者了解PCA的工作机制是什么。



PCA每个特征向量都是线性无关的

(1)最大方差理论

(2)最小化降维造成的损失



设有m条n维数据。

1）将原始数据按列组成n行m列矩阵X

2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值

3）求出协方差矩阵[![image](https://images2015.cnblogs.com/blog/361409/201607/361409-20160702074912781-1504440511.png)](http://images2015.cnblogs.com/blog/361409/201607/361409-20160702074912484-1826862146.png)

4）求出协方差矩阵的特征值及对应的特征向量

5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P

6）[![image](https://images2015.cnblogs.com/blog/361409/201607/361409-20160702074913515-1162408443.png)](http://images2015.cnblogs.com/blog/361409/201607/361409-20160702074913140-1136053910.png)即为降维到k维后的数据

**参考资料**

- [PCA的数学原理](https://www.cnblogs.com/mikewolf2002/p/3429711.html)
- [PCA的数学原理(转)](https://zhuanlan.zhihu.com/p/21580949)
- [第13章 利用 PCA 来简化数据](https://www.cnblogs.com/apachecnxy/p/7640976.html)
- [PCA（主成分分析）原理推导](https://zhuanlan.zhihu.com/p/84946694)

## 线性判别分析（LDA）

高耦合，底内聚

**参考资料**

- [线性判别分析LDA原理总结](https://www.cnblogs.com/pinard/p/6244265.html)
- [LDA原理小结](https://blog.csdn.net/qq_16137569/article/details/82385050)

## 奇异值分解（SVD）

### 基本原理

- [ ] TODO 

**参考资料**

- [Singular_value_decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)
- [关于SVD(Singular Value Decomposition)的那些事儿](https://www.cnblogs.com/tgycoder/p/6266786.html)
- [奇异值分解(SVD)原理与在降维中的应用](https://www.cnblogs.com/pinard/p/6251584.html)
- [第14章 利用SVD简化数据](https://www.cnblogs.com/apachecnxy/p/7640987.html)

### 手撕 SVD

- [ ] TODO

### 特征值和SVD的区别

- [ ] TODO

**参考资料**

- [特征值和奇异值（svd）](https://blog.csdn.net/u012380663/article/details/36629951)

## 凸优化

### 基本原理

- [ ] TODO 

**参考资料**

- [关于凸优化的一些简单概念](https://www.cnblogs.com/harvey888/p/7100815.html)
- [最优化理论与凸优化到底是干嘛的？](https://blog.csdn.net/qq_39422642/article/details/78816637)
- [深度学习/机器学习入门基础数学知识整理（三）：凸优化，Hessian，牛顿法](https://blog.csdn.net/xbinworld/article/details/79113218)

## 神经网络

- [ ] TODO 

注：建议放在深度学习中介绍，这里可以简单介绍

## Accuracy、Precision、Recall和 F1 Score

在学习机器学习、深度学习，甚至做自己项目的时候，经过看到上述名词。然而因为名词容易搞混，所以经常会忘记相关的含义。

这里做一次最全最清晰的介绍，若之后再次忘记相关知识点，本文可以帮助快速回顾。

首先，列出一个清单：

- TP（true positive，真正）: 预测为正，实际为正

- FP（false positive，假正）: 预测为正，实际为负

- TN（true negative，真负）：预测为负，实际为负

- FN（false negative，假负）: 预测为负，实际为正

- ACC（accuracy，准确率）：ACC = (TP+TN)/(TP+TN+FN+FP)

- P（precision精确率、精准率、查准率P = TP/ (TP+FP)

- R（recall，召回率、查全率）： R = TP/ (TP+FN)

- TPR（true positive rate，，真正类率同召回率、查全率）：TPR = TP/ (TP+FN)

  注：Recall = TPR

- FPR（false positive rate，假正类率）：FPR =FP/ (FP+TN)

- F-Score: F-Score = (1+β^2) x (PxR) / (β^2x(P+R)) = 2xTP/(2xTP + FP + FN)

- 当β=1是，F1-score = 2xPxR/(P+R)

- P-R曲线（precision-recall，查准率-查全率曲线）

- ROC曲线（receiver operating characteristic，接收者操作特征曲线）

- AUC（area under curve）值

中文博大精深，为了不搞混，下面统一用英文全称或简称作为名词标识。



正式介绍一下前四个名词：

**True positives（TP，真正）** : 预测为正，实际为正

**True negatives（TN，真负）**：预测为负，实际为负

**False positives（FP，假正**）: 预测为正，实际为负 

**False negatives（FN，假负）**: 预测为负，实际为正

为了更好的理解，这里二元分类问题的例子：

假设，我们要对某一封邮件做出一个判定，判定这封邮件是垃圾邮件、还是这封邮件不是垃圾邮件？

如果判定是垃圾邮件，那就是做出（Positive）的判定； 

如果判定不是垃圾邮件，那就做出（Negative）的判定。

True Positive（TP）意思表示做出Positive的判定，而且判定是正确的。

因此，TP的数值表示正确的Positive判定的个数。 

同理，False Positive（TP）数值表示错误的Positive判定的个数。 

依此，True Negative（TN）数值表示正确的Negative判定个数。 

False Negative（FN）数值表示错误的Negative判定个数。

**TPR、FPR和TNR**

**TPR（true positive rate，真正类率）**

TPR = TP/(TP+FN)

真正类率TPR代表分类器预测的正类中实际正实例占所有正实例的比例。



**FPR（false positive rate，假正类率）**

FPR = FP/(FP+TN)

假正类率FPR代表分类器预测的正类中实际负实例占所有负实例的比例。

**TNR（ture negative rate，真负类率）**

TNR = TN/(FP+TN)

真负类率TNR代表分类器预测的负类中实际负实例占所有负实例的比例。

**Accuracy**

准确率（accuracy，ACC）

ACC = (TP+TN)/(TP+TN+FN+FP)

**Precision & Recall**

[Precision精确率](https://en.wikipedia.org/wiki/Precision_and_recall)：

P = TP/(TP+FP)

表示当前划分到正样本类别中，被正确分类的比例（正确正样本所占比例）。

[Recall召回率](https://en.wikipedia.org/wiki/Precision_and_recall)：

R = TP/(TP+FN)

表示当前划分到正样本类别中，真实正样本占所有正样本的比例。

**F-Score**

F-Score 是精确率Precision和召回率Recall的加权调和平均值。该值是为了综合衡量Precision和Recall而设定的。

F-Score = (1+β^2) x (PxR) / (β^2x(P+R)) = 2xTP/(2xTP + FP + FN)

当β=1时，F1-score = 2xPxR/(P+R)。这时，Precision和Recall都很重要，权重相同。

当有些情况下，我们认为Precision更重要，那就调整β的值小于1；如果我们认为Recall更加重要，那就调整β的值大于1。

一般来说，当F-Score或F1-score较高

**P-R曲线**



**ROC曲线**

横轴：负正类率(false postive rate FPR)

纵轴：真正类率(true postive rate TPR)



![ROC Curve](https://upload-images.jianshu.io/upload_images/2394427-5f11fd1e6af07393?imageMogr2/auto-orient/strip%7CimageView2/2/w/700)

**AUC值**

上面都是理论，看起来很迷糊，这里举个真实应用的实例，加强理解。

对于那些不熟悉的人，我将解释精确度和召回率，对于那些熟悉的人，我将在比较精确召回曲线时解释文献中的一些混淆。

下面从图像分类的角度举个例子：

假设现在有这样一个测试集，测试集中的图片只由大雁和飞机两种图片组成，如下图所示： 

![](https://sanchom.files.wordpress.com/2011/08/collection.png)

假设你的分类系统最终的目的是：能取出测试集中所有飞机的图片，而不是大雁的图片。

现在做如下的定义： 

True positives（TP，真正） : 飞机的图片被正确的识别成了飞机。 

True negatives（TN，真负）: 大雁的图片没有被识别出来，系统正确地认为它们是大雁。 

False positives（FP，假正）: 大雁的图片被错误地识别成了飞机。 

False negatives（FN，假负）: 飞机的图片没有被识别出来，系统错误地认为它们是大雁。

![Precision and recall](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/440px-Precisionrecall.svg.png)

**实战**

```python
'''In binary classification settings'''

######### Create simple data ##########

from sklearn import svm, datasets
from sklearn.model_selection import train_test_split
import numpy as np

iris = datasets.load_iris()
X = iris.data
y = iris.target

# Add noisy features
random_state = np.random.RandomState(0)
n_samples, n_features = X.shape
X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]

# Limit to the two first classes, and split into training and test
X_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],
                                                    test_size=.5,
                                                    random_state=random_state)

# Create a simple classifier
classifier = svm.LinearSVC(random_state=random_state)
classifier.fit(X_train, y_train)
y_score = classifier.decision_function(X_test)


######## Compute the average precision score ######## 

from sklearn.metrics import average_precision_score
average_precision = average_precision_score(y_test, y_score)

print('Average precision-recall score: {0:0.2f}'.format(
      average_precision))
	  

######## Plot the Precision-Recall curve   ######
	  
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

precision, recall, _ = precision_recall_curve(y_test, y_score)

plt.step(recall, precision, color='b', alpha=0.2,
         where='post')
plt.fill_between(recall, precision, step='post', alpha=0.2,
                 color='b')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])
plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(
          average_precision))
plt.show()
```

**参考资料**

- [Accuracy, Precision, Recall & F1 Score: Interpretation of Performance Measures](http://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)

- [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)

- [average precision](https://sanchom.wordpress.com/tag/average-precision/)

- [Precision-Recall](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)

- [【YOLO学习】召回率（Recall），精确率（Precision），平均正确率（Average_precision(AP) ），交除并（Intersection-over-Union（IoU））](https://blog.csdn.net/hysteric314/article/details/54093734)

- [Precision，Recall，F1score，Accuracy的理解](https://blog.csdn.net/u014380165/article/details/77493978)

- [ROC、Precision、Recall、TPR、FPR理解](https://www.jianshu.com/p/be2e037900a1)

- [推荐系统评测指标—准确率(Precision)、召回率(Recall)、F值(F-Measure) ](http://bookshadow.com/weblog/2014/06/10/precision-recall-f-measure/)

- [机器学习之分类器性能指标之ROC曲线、AUC值](http://www.cnblogs.com/dlml/p/4403482.html)

## 正则化方法

- L1 范数
- L2 范数
- 数据集增广

- Dropout
- Batch Normaliztion

**参考资料**

- [[Deep Learning\] 正则化](https://www.cnblogs.com/maybe2030/p/9231231.html)

### L1和L2正则化

目的：降低损失函数

机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作ℓ1-norm和ℓ2-norm，中文称作L1正则化和L2正则化，或者L1范数和L2范数。

L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制。对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）。

L2到底有没有平方根（可以没有，不影响什么）：https://stats.stackexchange.com/a/449755

下图是Python中Ridge回归的损失函数，式中加号后面一项即为L2正则化项。

一般回归分析中回归w表示特征的系数，从上式可以看到正则化项是对系数做了处理（限制）。L1正则化和L2正则化的说明如下：

- L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为||w||1
- L2正则化是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为||w||2
  一般都会在正则化项之前添加一个系数，Python中用α表示，一些文章也用λ表示。这个系数需要用户指定。

那添加L1和L2正则化有什么用？下面是L1正则化和L2正则化的作用，这些表述可以在很多文章中找到。

- L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择
- L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合

**稀疏模型与特征选择**

上面提到L1正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。为什么要生成一个稀疏矩阵？

稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0. 通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。这就是稀疏模型与特征选择的关系。

**L1和L2正则化的直观理解**

见https://zhuanlan.zhihu.com/p/30394026

注：以二维平面举例，借助可视化L1和L2，可知L1正则化具有稀疏性。

### weight decay

https://blog.csdn.net/program_developer/article/details/80867468#:~:text=weight%20decay%E7%9A%84%E4%BD%9C%E7%94%A8%E4%B8%BB%E8%A6%81,%E6%8B%9F%E5%90%88%E6%AD%A3%E5%B8%B8%E8%BE%93%E5%85%A5weight

## 过拟合和欠拟合

### 基本原理

**过拟合（Over-Fitting）**

高方差

在训练集上误差小，但在测试集上误差大，我们将这种情况称为高方差（high variance），也叫过拟合。

**欠拟合（Under-Fitting）**

在训练集上训练效果不好（测试集上也不好），准确率不高，我们将这种情况称为高偏差（high bias），也叫欠拟合。

![](https://testerhome.com/uploads/photo/2017/ba5ebeb8-1af7-4dfa-aeba-6bb36c056aff.png!large)



### 如何防止过拟合？

- 数据增广（Data Augmentation）
- 正则化（L0正则、L1正则和L2正则），也叫限制权值Weight-decay
- Dropout
- Early Stopping
- 简化模型
- 增加噪声
- Bagging
- 贝叶斯方法
- 决策树剪枝
- 集成方法，随机森林
- Batch Normalization

### 如何防止欠拟合？

- 添加新特征
- 添加多项式特征
- 减少正则化参数
- 增加网络复杂度
- 使用集成学习方法，如Bagging

## AUC 和 ROC

### 基本原理

ROC 曲线和 AUC 常被用来评价一个二值分类器的优劣。横坐标为假阳性率，纵坐标为真阳性率。因此，ROC 曲线越接近左上角，分类器的性能越好。AUC是 ROC 曲线下的面积，它是一个数值，沿着 ROC 横轴做积分，当仅仅看 ROC 曲线分辨不出哪个分类器的效果更好时，用这个数值来判断。随机挑选一个正样本和一个负样本，当前分类算法得到的 Score 将这个正样本排在负样本前面的概率就是 AUC 值。AUC 值是一个概率值，取值一般在 0.5～1 之间，AUC 值越大，分类算法越好。

### 如何绘制ROC曲线？

按 Score 从大到小排列

依次将每个 Score 设定为阈值，然后这 20 个样本的标签会变化，当它的 score 大于或等于当前阈值时，则为正样本，否则为负样本。

这样对每个阈值，可以计算一组 FPR 和 TPR，此例一共可以得到 20 组。

当阈值设置为 1 和 0 时， 可以得到 ROC 曲线上的 (0,0) 和 (1,1) 两个点。



作者：不会停的蜗牛
链接：https://www.jianshu.com/p/ab7818f6a5e5
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

### ROC曲线下面积表示什么？

AUC

### 手撕 AUC 曲面面积代码

在牛客看到面经说手撕ＡＵＣ，所以实现一下。首先要明白ＡＵＣ的物理含义不仅是ＲＯＣ曲线下的面积，ＡＵＣ还有另外一个物理含义就是：给定正样本Ｍ个，负样本Ｎ个，以及他们的预测概率（０－１）之间，那么ＡＵＣ的含义就是所有穷举所有的正负样本对，如果正样本的预测概率大于负样本的预测概率，那么就＋１；如果如果正样本的预测概率等于负样本的预测概率，那么就＋0.5,　如果正样本的预测概率小于负样本的预测概率，那么就＋０；最后把统计处理的个数除以Ｍ×Ｎ就得到我们的ＡＵＣ

https://zhuanlan.zhihu.com/p/84035782

## 梯度弥散和梯度爆炸

反向传播基于的是链式求导法则。如果导数小于1，那么随着层数的增多，梯度的更新量会以指数形式衰减，结果就是越靠近输出层的网络层参数更新比较正常，而靠近输入层的网络层参数可能基本就不更新。这就是梯度消失。而如果导数值大于1，那么由于链式法则的连乘，梯度更新量是会成指数级增长的。这就是梯度爆炸。

### 梯度消失、爆炸的解决方案

1. 预训练+微调
2. 梯度剪切+正则
3. relu等激活函数
4. batchnormalization
5. resnet

## 什么是参数范数惩罚？

就是L1，L2

## 为什么要进行归一化？优点？

1. 在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度

2. 提升模型的精度

   归一化的另一好处是提高精度，这在涉及到一些距离计算的算法时效果显著，比如算法要计算欧氏距离，上图中x2的取值范围比较小，涉及到距离计算时其对结果的影响远比x1带来的小，所以这就会造成精度的损失。所以归一化很有必要，他可以让各个特征对结果做出的贡献相同。

## CCA和PCA的区别

- [ ] TODO

## Softmax

### 基本原理

- [ ] TODO

### Softmax是和什么loss function配合使用？

交叉熵

###  Softmax代码实现

- [ ] TODO

## 交叉熵损失函数

- [ ] TODO

## 通俗解释一下信息熵

- [ ] TODO

## 如何加快梯度下降收敛速度？

1. mini-batch

   所以mini-batch比传统的梯度下降法下降的速度快，但是mini-batch的cost曲线没有传统梯度下降法的cost曲线光滑

2. Stochastic gradient descent

   Stochastic gradient descent可以看做是mini-batch的一种特殊情况，当mini-batch size等于1时，mini-batch就退化为Stochastic gradient descent

3. 利用monmentum或RMSprop或Adam方法求得梯度的变体，然后利用该变体更新参数即可。

## 如何解决正负样本数量不均衡？

1. 上采样/下采样

2. 数据合成

3. 异常检测

4. class weight：给错误估计的小样本数据更大的loss惩罚

   In binary classification, class weights could be represented just by **calculating the frequency of the positive and negative class and then inverting it** so that when multiplied to the class loss, the underrepresented class has a much higher error than the majority class.

## 如何解决异常值问题？

这个原则有个条件：**数据需要服从正态分布。**在3∂原则下，异常值如超过3倍标准差，那么可以将其视为异常值

- **删除含有异常值的记录：**直接将含有异常值的记录删除；
- **视为缺失值：**将异常值视为缺失值，利用缺失值处理的方法进行处理；
- **平均值修正：**可用前后两个观测值的平均值修正该异常值；
- **不处理：**直接在具有异常值的数据集上进行数据挖掘；

是否要删除异常值可根据实际情况考虑。因为一些模型对异常值不很敏感，即使有异常值也不影响模型效果，但是一些模型比如逻辑回归LR对异常值很敏感，如果不进行处理，可能会出现过拟合等非常差的效果。

> ## 机器学习中使用正则化来防止过拟合是什么原理？
>
> - [ ] TODO
>
> **参考资料**
>
> - [机器学习中使用正则化来防止过拟合是什么原理？](https://www.zhihu.com/question/20700829)
>
> ## 机器学习中常常提到的正则化到底是什么意思？
>
> - [ ] TODO
>
> **参考资料**
>
> - [机器学习中常常提到的正则化到底是什么意思？](https://www.zhihu.com/question/20924039)
>

## 梯度下降陷入局部最优有什么解决办法

1. 自适应学习率
2. SGD，mini-batch GD
3. 动量

## 聚类算法中的距离度量有哪些？

- [ ] TODO

常见的距离度量：欧氏距离、曼哈顿距离、夹角余弦、切比雪夫距离、汉明距离

## KL 散度和交叉熵的区别

A和B的交叉熵等于A与B的KL散度减去A的熵。对于训练模型来说，训练数据A已知的情况下，KL散度等于交叉熵。

## 降维的方法都有哪些？

- [ ] TODO

## TODO

